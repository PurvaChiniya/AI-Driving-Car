{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "data = pd.read_csv(\"/home/vader/Downloads/testdata.csv\")\n",
    "cmddata = pd.read_csv(\"/home/vader/Downloads/testdatacmd.csv\")\n",
    "Y=cmddata.cmd\n",
    "X=data.training\n",
    "train=data.loc[:,\"training\"].values\n",
    "train\n",
    "S1=[]\n",
    "S2=[]\n",
    "S3=[]\n",
    "cmnd=[]\n",
    "ctrl=[]\n",
    "counter=0;\n",
    "for i in train:\n",
    "    counter=counter+1;\n",
    "    if(counter%4==1):\n",
    "        cmnd.append(i)\n",
    "    if(counter%4==2):\n",
    "        S1.append(i)\n",
    "    if(counter%4==3):\n",
    "        S2.append(i)\n",
    "    if(counter%4==0):\n",
    "        S3.append(i) \n",
    "S1=np.int_(S1)\n",
    "S2=np.int_(S2)\n",
    "S3=np.int_(S3)        \n",
    "        \n",
    "np.savetxt(\"/home/vader/Downloads/sens1.csv\", S1, delimiter=\",\")\n",
    "np.savetxt(\"/home/vader/Downloads/sens2.csv\", S2, delimiter=\",\")\n",
    "np.savetxt(\"/home/vader/Downloads/sens3.csv\", S3, delimiter=\",\")        \n",
    "with open(\"/home/vader/Downloads/ctrl.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in cmnd:\n",
    "        writer.writerow([val])\n",
    "for i in cmnd:\n",
    "    if(i=='Stop'):\n",
    "        ctrl.append(0)\n",
    "    elif(i=='Forward'):\n",
    "        ctrl.append(1)    \n",
    "    elif(i=='Left'):\n",
    "        ctrl.append(2)\n",
    "    elif(i=='Right'):\n",
    "        ctrl.append(3)\n",
    "    elif(i=='Backward'):\n",
    "        ctrl.append(4)\n",
    "np.savetxt(\"/home/vader/Downloads/ctrlnum.csv\", ctrl, delimiter=\",\") \n",
    "counter=0;\n",
    "S7=[]\n",
    "S5=[]\n",
    "S6=[]\n",
    "cmnd2=[]\n",
    "ctrl2=[]\n",
    "Flag=0\n",
    "for i in train:\n",
    "    counter=counter+1;\n",
    "    if(counter%4==1):\n",
    "        if(i=='Stop'):\n",
    "            Flag=1\n",
    "        else:\n",
    "            cmnd2.append(i)\n",
    "            Flag=0\n",
    "    if(counter%4==2 and Flag==0):\n",
    "        S5.append(i)\n",
    "    if(counter%4==3 and Flag==0):\n",
    "        S6.append(i)\n",
    "    if(counter%4==0 and Flag==0):\n",
    "        S7.append(i) \n",
    "S5=np.int_(S5)\n",
    "S6=np.int_(S6)\n",
    "S7=np.int_(S7)\n",
    "np.savetxt(\"/home/vader/Downloads/sens5.csv\", S5, delimiter=\",\")\n",
    "np.savetxt(\"/home/vader/Downloads/sens6.csv\", S6, delimiter=\",\")\n",
    "np.savetxt(\"/home/vader/Downloads/sens7.csv\", S7, delimiter=\",\")        \n",
    "with open(\"/home/vader/Downloads/ctrl2.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in cmnd:\n",
    "        writer.writerow([val])\n",
    "for i in cmnd2:\n",
    "    if(i=='Stop'):\n",
    "        ctrl2.append(0)\n",
    "    elif(i=='Forward'):\n",
    "        ctrl2.append(1)    \n",
    "    elif(i=='Left'):\n",
    "        ctrl2.append(2)\n",
    "    elif(i=='Right'):\n",
    "        ctrl2.append(3)\n",
    "    elif(i=='Backward'):\n",
    "        ctrl2.append(4)\n",
    "np.savetxt(\"/home/vader/Downloads/ctrlnum2.csv\", ctrl2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.read_csv(\"/home/vader/Downloads/sens1.csv\")\n",
    "B = pd.read_csv(\"/home/vader/Downloads/sens2.csv\")\n",
    "C = pd.read_csv(\"/home/vader/Downloads/sens3.csv\")\n",
    "X = np.hstack([A,B,C])\n",
    "y = pd.read_csv(\"/home/vader/Downloads/ctrlnum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([A,B,C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vader/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=.2,random_state=3)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr= LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5621198957428323"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class=lgr.predict(X_test)\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5771739130434783"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class2=lgr.predict(X_train)\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_train,y_pred_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "4600/4600 [==============================] - 0s 101us/step - loss: 1.4709 - acc: 0.3937\n",
      "Epoch 2/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.2021 - acc: 0.5750\n",
      "Epoch 3/21\n",
      "4600/4600 [==============================] - 0s 70us/step - loss: 1.1861 - acc: 0.5750\n",
      "Epoch 4/21\n",
      "4600/4600 [==============================] - 0s 58us/step - loss: 1.1646 - acc: 0.5750\n",
      "Epoch 5/21\n",
      "4600/4600 [==============================] - 0s 58us/step - loss: 1.1488 - acc: 0.5746\n",
      "Epoch 6/21\n",
      "4600/4600 [==============================] - 0s 59us/step - loss: 1.1390 - acc: 0.5783\n",
      "Epoch 7/21\n",
      "4600/4600 [==============================] - 0s 106us/step - loss: 1.1280 - acc: 0.5796\n",
      "Epoch 8/21\n",
      "4600/4600 [==============================] - 0s 71us/step - loss: 1.1176 - acc: 0.5926\n",
      "Epoch 9/21\n",
      "4600/4600 [==============================] - 1s 109us/step - loss: 1.1083 - acc: 0.5998\n",
      "Epoch 10/21\n",
      "4600/4600 [==============================] - 0s 64us/step - loss: 1.1056 - acc: 0.6028\n",
      "Epoch 11/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.0977 - acc: 0.6041\n",
      "Epoch 12/21\n",
      "4600/4600 [==============================] - 0s 59us/step - loss: 1.0939 - acc: 0.6020\n",
      "Epoch 13/21\n",
      "4600/4600 [==============================] - 0s 59us/step - loss: 1.0893 - acc: 0.6026\n",
      "Epoch 14/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.0844 - acc: 0.6072\n",
      "Epoch 15/21\n",
      "4600/4600 [==============================] - 0s 61us/step - loss: 1.0791 - acc: 0.6061\n",
      "Epoch 16/21\n",
      "4600/4600 [==============================] - 0s 59us/step - loss: 1.0822 - acc: 0.6072\n",
      "Epoch 17/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.0795 - acc: 0.6070\n",
      "Epoch 18/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.0754 - acc: 0.6120\n",
      "Epoch 19/21\n",
      "4600/4600 [==============================] - 0s 60us/step - loss: 1.0742 - acc: 0.6141\n",
      "Epoch 20/21\n",
      "4600/4600 [==============================] - 0s 61us/step - loss: 1.0704 - acc: 0.6117\n",
      "Epoch 21/21\n",
      "4600/4600 [==============================] - 0s 61us/step - loss: 1.0653 - acc: 0.6139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6177237185056472"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu',input_dim=3))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=21, batch_size=16)\n",
    "y_pred =model.predict_classes(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2458"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmnd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2458"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(S5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2457, 3)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "      \n",
    "A = pd.read_csv(\"/home/vader/Downloads/sens5.csv\")\n",
    "B = pd.read_csv(\"/home/vader/Downloads/sens6.csv\")\n",
    "C = pd.read_csv(\"/home/vader/Downloads/sens7.csv\")\n",
    "X = np.hstack([A,B,C])\n",
    "y = pd.read_csv(\"/home/vader/Downloads/ctrlnum2.csv\")\n",
    "X = np.hstack([A,B,C])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vader/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5022900763358779"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=.2,random_state=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr= LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "y_pred_class2=lgr.predict(X_train)\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_train,y_pred_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1965/1965 [==============================] - 0s 189us/step - loss: 1.4452 - acc: 0.3842\n",
      "Epoch 2/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.3123 - acc: 0.4494\n",
      "Epoch 3/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.2393 - acc: 0.5216\n",
      "Epoch 4/100\n",
      "1965/1965 [==============================] - 0s 73us/step - loss: 1.2042 - acc: 0.5242\n",
      "Epoch 5/100\n",
      "1965/1965 [==============================] - 0s 65us/step - loss: 1.1948 - acc: 0.5160\n",
      "Epoch 6/100\n",
      "1965/1965 [==============================] - 0s 63us/step - loss: 1.1722 - acc: 0.5237\n",
      "Epoch 7/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.1591 - acc: 0.5262\n",
      "Epoch 8/100\n",
      "1965/1965 [==============================] - 0s 63us/step - loss: 1.1523 - acc: 0.5298\n",
      "Epoch 9/100\n",
      "1965/1965 [==============================] - 0s 63us/step - loss: 1.1531 - acc: 0.5181\n",
      "Epoch 10/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.1420 - acc: 0.5303\n",
      "Epoch 11/100\n",
      "1965/1965 [==============================] - 0s 62us/step - loss: 1.1370 - acc: 0.5349\n",
      "Epoch 12/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.1444 - acc: 0.5221\n",
      "Epoch 13/100\n",
      "1965/1965 [==============================] - 0s 61us/step - loss: 1.1284 - acc: 0.5282\n",
      "Epoch 14/100\n",
      "1965/1965 [==============================] - 0s 115us/step - loss: 1.1248 - acc: 0.5298\n",
      "Epoch 15/100\n",
      "1965/1965 [==============================] - 0s 96us/step - loss: 1.1213 - acc: 0.5232\n",
      "Epoch 16/100\n",
      "1965/1965 [==============================] - 0s 81us/step - loss: 1.1181 - acc: 0.5242\n",
      "Epoch 17/100\n",
      "1965/1965 [==============================] - 0s 73us/step - loss: 1.1219 - acc: 0.5282\n",
      "Epoch 18/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.1192 - acc: 0.5277\n",
      "Epoch 19/100\n",
      "1965/1965 [==============================] - 0s 78us/step - loss: 1.1193 - acc: 0.5226\n",
      "Epoch 20/100\n",
      "1965/1965 [==============================] - 0s 74us/step - loss: 1.1068 - acc: 0.5450\n",
      "Epoch 21/100\n",
      "1965/1965 [==============================] - 0s 122us/step - loss: 1.1064 - acc: 0.5471\n",
      "Epoch 22/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0995 - acc: 0.5425\n",
      "Epoch 23/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0999 - acc: 0.5517\n",
      "Epoch 24/100\n",
      "1965/1965 [==============================] - 0s 68us/step - loss: 1.0988 - acc: 0.5466\n",
      "Epoch 25/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0962 - acc: 0.5517\n",
      "Epoch 26/100\n",
      "1965/1965 [==============================] - 0s 77us/step - loss: 1.0929 - acc: 0.5562\n",
      "Epoch 27/100\n",
      "1965/1965 [==============================] - 0s 84us/step - loss: 1.0829 - acc: 0.5537\n",
      "Epoch 28/100\n",
      "1965/1965 [==============================] - 0s 75us/step - loss: 1.0900 - acc: 0.5537\n",
      "Epoch 29/100\n",
      "1965/1965 [==============================] - 0s 74us/step - loss: 1.0899 - acc: 0.5557\n",
      "Epoch 30/100\n",
      "1965/1965 [==============================] - 0s 75us/step - loss: 1.0839 - acc: 0.5674\n",
      "Epoch 31/100\n",
      "1965/1965 [==============================] - 0s 82us/step - loss: 1.0922 - acc: 0.5603\n",
      "Epoch 32/100\n",
      "1965/1965 [==============================] - 0s 83us/step - loss: 1.0776 - acc: 0.5593\n",
      "Epoch 33/100\n",
      "1965/1965 [==============================] - 0s 82us/step - loss: 1.0763 - acc: 0.5639\n",
      "Epoch 34/100\n",
      "1965/1965 [==============================] - 0s 74us/step - loss: 1.0895 - acc: 0.5506\n",
      "Epoch 35/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0761 - acc: 0.5618\n",
      "Epoch 36/100\n",
      "1965/1965 [==============================] - 0s 71us/step - loss: 1.0768 - acc: 0.5578\n",
      "Epoch 37/100\n",
      "1965/1965 [==============================] - 0s 69us/step - loss: 1.0686 - acc: 0.5674\n",
      "Epoch 38/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0781 - acc: 0.5588\n",
      "Epoch 39/100\n",
      "1965/1965 [==============================] - 0s 72us/step - loss: 1.0725 - acc: 0.5628\n",
      "Epoch 40/100\n",
      "1965/1965 [==============================] - 0s 136us/step - loss: 1.0696 - acc: 0.5649\n",
      "Epoch 41/100\n",
      "1965/1965 [==============================] - 0s 135us/step - loss: 1.0696 - acc: 0.5644\n",
      "Epoch 42/100\n",
      "1965/1965 [==============================] - 0s 71us/step - loss: 1.0749 - acc: 0.5588\n",
      "Epoch 43/100\n",
      "1965/1965 [==============================] - 0s 121us/step - loss: 1.0715 - acc: 0.5598\n",
      "Epoch 44/100\n",
      "1965/1965 [==============================] - 0s 129us/step - loss: 1.0656 - acc: 0.5588\n",
      "Epoch 45/100\n",
      "1965/1965 [==============================] - 0s 75us/step - loss: 1.0647 - acc: 0.5623\n",
      "Epoch 46/100\n",
      "1965/1965 [==============================] - 0s 62us/step - loss: 1.0545 - acc: 0.5720\n",
      "Epoch 47/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0606 - acc: 0.5588\n",
      "Epoch 48/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.0543 - acc: 0.5644\n",
      "Epoch 49/100\n",
      "1965/1965 [==============================] - 0s 68us/step - loss: 1.0576 - acc: 0.5669\n",
      "Epoch 50/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0561 - acc: 0.5684\n",
      "Epoch 51/100\n",
      "1965/1965 [==============================] - 0s 65us/step - loss: 1.0643 - acc: 0.5573\n",
      "Epoch 52/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0495 - acc: 0.5705\n",
      "Epoch 53/100\n",
      "1965/1965 [==============================] - 0s 75us/step - loss: 1.0498 - acc: 0.5644\n",
      "Epoch 54/100\n",
      "1965/1965 [==============================] - 0s 72us/step - loss: 1.0439 - acc: 0.5710\n",
      "Epoch 55/100\n",
      "1965/1965 [==============================] - 0s 64us/step - loss: 1.0465 - acc: 0.5725\n",
      "Epoch 56/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0457 - acc: 0.5679\n",
      "Epoch 57/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0479 - acc: 0.5649\n",
      "Epoch 58/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.0330 - acc: 0.5669\n",
      "Epoch 59/100\n",
      "1965/1965 [==============================] - 0s 69us/step - loss: 1.0462 - acc: 0.5618\n",
      "Epoch 60/100\n",
      "1965/1965 [==============================] - 0s 64us/step - loss: 1.0417 - acc: 0.5634\n",
      "Epoch 61/100\n",
      "1965/1965 [==============================] - 0s 68us/step - loss: 1.0373 - acc: 0.5669\n",
      "Epoch 62/100\n",
      "1965/1965 [==============================] - 0s 72us/step - loss: 1.0412 - acc: 0.5639\n",
      "Epoch 63/100\n",
      "1965/1965 [==============================] - 0s 105us/step - loss: 1.0479 - acc: 0.5542\n",
      "Epoch 64/100\n",
      "1965/1965 [==============================] - 0s 71us/step - loss: 1.0351 - acc: 0.5628\n",
      "Epoch 65/100\n",
      "1965/1965 [==============================] - 0s 64us/step - loss: 1.0362 - acc: 0.5705\n",
      "Epoch 66/100\n",
      "1965/1965 [==============================] - 0s 102us/step - loss: 1.0360 - acc: 0.5725\n",
      "Epoch 67/100\n",
      "1965/1965 [==============================] - 0s 117us/step - loss: 1.0493 - acc: 0.5583\n",
      "Epoch 68/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.0267 - acc: 0.5669\n",
      "Epoch 69/100\n",
      "1965/1965 [==============================] - 0s 86us/step - loss: 1.0411 - acc: 0.5476\n",
      "Epoch 70/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.0359 - acc: 0.5690\n",
      "Epoch 71/100\n",
      "1965/1965 [==============================] - 0s 98us/step - loss: 1.0237 - acc: 0.5700\n",
      "Epoch 72/100\n",
      "1965/1965 [==============================] - 0s 71us/step - loss: 1.0176 - acc: 0.5791\n",
      "Epoch 73/100\n",
      "1965/1965 [==============================] - 0s 66us/step - loss: 1.0344 - acc: 0.5766\n",
      "Epoch 74/100\n",
      "1965/1965 [==============================] - 0s 64us/step - loss: 1.0240 - acc: 0.5776\n",
      "Epoch 75/100\n",
      "1965/1965 [==============================] - 0s 72us/step - loss: 1.0392 - acc: 0.5639\n",
      "Epoch 76/100\n",
      "1965/1965 [==============================] - 0s 80us/step - loss: 1.0296 - acc: 0.5735\n",
      "Epoch 77/100\n",
      "1965/1965 [==============================] - 0s 77us/step - loss: 1.0155 - acc: 0.5807\n",
      "Epoch 78/100\n",
      "1965/1965 [==============================] - 0s 68us/step - loss: 1.0046 - acc: 0.5868\n",
      "Epoch 79/100\n",
      "1965/1965 [==============================] - 0s 67us/step - loss: 1.0190 - acc: 0.5761\n",
      "Epoch 80/100\n",
      "1965/1965 [==============================] - 0s 72us/step - loss: 1.0314 - acc: 0.5756\n",
      "Epoch 81/100\n",
      "1965/1965 [==============================] - 0s 69us/step - loss: 1.0166 - acc: 0.5908\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965/1965 [==============================] - 0s 71us/step - loss: 1.0227 - acc: 0.5771\n",
      "Epoch 83/100\n",
      "1965/1965 [==============================] - 0s 65us/step - loss: 1.0339 - acc: 0.5832\n",
      "Epoch 84/100\n",
      "1965/1965 [==============================] - 0s 61us/step - loss: 1.0177 - acc: 0.5847\n",
      "Epoch 85/100\n",
      "1965/1965 [==============================] - 0s 88us/step - loss: 1.0110 - acc: 0.5954\n",
      "Epoch 86/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.0203 - acc: 0.5944\n",
      "Epoch 87/100\n",
      "1965/1965 [==============================] - 0s 85us/step - loss: 1.0074 - acc: 0.5964\n",
      "Epoch 88/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.0035 - acc: 0.5908\n",
      "Epoch 89/100\n",
      "1965/1965 [==============================] - 0s 73us/step - loss: 1.0180 - acc: 0.5852\n",
      "Epoch 90/100\n",
      "1965/1965 [==============================] - 0s 70us/step - loss: 1.0166 - acc: 0.5863\n",
      "Epoch 91/100\n",
      "1965/1965 [==============================] - 0s 79us/step - loss: 1.0277 - acc: 0.5842\n",
      "Epoch 92/100\n",
      "1965/1965 [==============================] - 0s 76us/step - loss: 1.0064 - acc: 0.6056\n",
      "Epoch 93/100\n",
      "1965/1965 [==============================] - 0s 82us/step - loss: 1.0150 - acc: 0.6015\n",
      "Epoch 94/100\n",
      "1965/1965 [==============================] - 0s 95us/step - loss: 1.0099 - acc: 0.5929\n",
      "Epoch 95/100\n",
      "1965/1965 [==============================] - 0s 73us/step - loss: 1.0117 - acc: 0.5975\n",
      "Epoch 96/100\n",
      "1965/1965 [==============================] - 0s 69us/step - loss: 1.0012 - acc: 0.6107\n",
      "Epoch 97/100\n",
      "1965/1965 [==============================] - 0s 83us/step - loss: 1.0113 - acc: 0.6097\n",
      "Epoch 98/100\n",
      "1965/1965 [==============================] - 0s 81us/step - loss: 1.0015 - acc: 0.6122\n",
      "Epoch 99/100\n",
      "1965/1965 [==============================] - 0s 90us/step - loss: 1.0032 - acc: 0.6061\n",
      "Epoch 100/100\n",
      "1965/1965 [==============================] - 0s 82us/step - loss: 0.9937 - acc: 0.6173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5894308943089431"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu',input_dim=3))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=100, batch_size=16)\n",
    "y_pred =model.predict_classes(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
